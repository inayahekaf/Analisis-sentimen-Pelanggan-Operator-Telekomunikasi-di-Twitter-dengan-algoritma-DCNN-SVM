# -*- coding: utf-8 -*-
"""DCNN-SVM INDOSAT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pJ--4mgefLAkoapYTTE9nO8JDk19oXaS
"""

import os
from google.colab import drive
drive.mount('/content/gdrive')
data_dir=os.path.join('gdrive','My Drive','Tweepy','BISMILLAH TA','indosat')
data_file=os.path.join(data_dir, 'indosatallclean')

import pandas as pd
import csv
df = pd.read_csv('gdrive/My Drive/tweepy/BISMILLAH TA/data/new_scrapping/1000data.csv',encoding='ISO-8859-1', sep=';',header = None, names=['text','label'])
df

testing = df.text[:11000]
testing

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
def token(test_result):
  tokenized_sents = [word_tokenize(i) for i in testing]
  for i in tokenized_sents:
      print (i)
  return tokenized_sents

k = token(testing)

import matplotlib.pyplot as plt, re
import warnings; warnings.simplefilter('ignore')
import numpy as np, pandas as pd, seaborn as sns; sns.set()
import tensorflow as tf
from tqdm import tqdm
from tensorflow.python.client import device_lib
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers.embeddings import Embedding
from keras import layers
import keras.backend as K
from keras.optimizers import Adam
from keras.models import Sequential, Input, Model
from keras.layers import Dense, LSTM, Dropout, Activation, Flatten, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Concatenate
from sklearn.datasets import fetch_20newsgroups
from nltk import sent_tokenize
from textblob import TextBlob
from sklearn.svm import LinearSVC, SVC
from sklearn import svm, metrics
from keras.callbacks import ModelCheckpoint
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report
from sklearn.model_selection import cross_val_score

import random
from gensim.models.word2vec import Word2Vec
word_vectors = Word2Vec.load('gdrive/My Drive/tweepy/BISMILLAH TA/data/data_word2vec/model_w2v_databaru.model')
#word_vectors = Word2Vec.load('gdrive/My Drive/tweepy/BISMILLAH TA/data/data_word2vec/model_w2v_xl24juli.model')

max_len = max(len(kata) for kata in k)
max_len

#CNN PARAMETER
maxlen = max_len
batch_size = 10
embedding_dims = 50
filters = 100
kernel_size3 = 5
kernel_size2 = 2
kernel_size1 = 7
hidden_dims = 100
epoch =30

print(word_vectors.vocabulary)

def vectorize(dataset, maxlen):
  vectorized_data=[]
  expected=[]
  for kalimat in dataset:
    sample_vecs=[]
    for kata in kalimat:
      if kata in word_vectors:
        sample_vecs.append(word_vectors[kata])
      else:
        sample_vecs.append(np.random.uniform(0.0,0.0,embedding_dims))
    if len(sample_vecs) < maxlen:
      additional_elems = maxlen- len(sample_vecs)
      for _ in range (additional_elems):
        sample_vecs.append(np.random.uniform(0.0,0.0,embedding_dims))
    sample_vecs=np.array(sample_vecs)
    vectorized_data.append(sample_vecs)
  vectorized_data = np.array(vectorized_data)
  return vectorized_data, sample_vecs

vectorize_data, sample_vecs = vectorize(k, maxlen)

split_point = int(len(k)*.8)
x_train = vectorize_data[:split_point]
y_train = df.label[:split_point]
x_test = vectorize_data[split_point:]
y_test = df.label[split_point:]
a = len(y_train[y_train==-1])
b = len(y_train[y_train==1])
c = len(y_test[y_test==-1])
d = len(y_test[y_test==1])
 
print('training negatif '+str(a))
print('training positif '+str(b))
print('testing negatif '+str(c))
print('testing positif '+str(d))

Y_train=pd.get_dummies(y_train).values
Y_test=pd.get_dummies(y_test).values
Y_train

x_train.shape

# model = Sequential() 
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
# model.add(MaxPooling1D(pool_size=2, strides=1))         
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides=1,input_shape=(maxlen,embedding_dims)))
# model.add(MaxPooling1D(pool_size=2, strides=1)) 
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides=1,input_shape=(maxlen,embedding_dims)))
# model.add(MaxPooling1D(pool_size=2, strides=1))
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides=1,input_shape=(maxlen,embedding_dims)))
# # model.add(GlobalMaxPooling1D())
# model.add(Flatten())
# model.add(Dense(hidden_dims))
# model.add(Dropout(0.3))
# adam = Adam(lr=0.00000001)
# model.add(Dense(2,activation = 'sigmoid'))
# # weight = model.layers[8].get_weights()
# # print(weight)
# # print(model.get_weights())
# # model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])

model = Sequential() 
model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
model.add(MaxPooling1D(pool_size=2, strides=1))         
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides=1,input_shape=(maxlen,embedding_dims)))
# model.add(MaxPooling1D(pool_size=2, strides=1)) 
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides=1,input_shape=(maxlen,embedding_dims)))
# model.add(MaxPooling1D(pool_size=2, strides=1))
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides=1,input_shape=(maxlen,embedding_dims)))
# model.add(GlobalMaxPooling1D())
model.add(Flatten())
model.add(Dense(hidden_dims))
model.add(Dropout(0.3))
adam = Adam(lr=0.00000001)
model.add(Dense(3,activation = 'softmax'))
print(model.get_weights())
# model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])

# model = Sequential()
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
# convout1 = Activation('relu')
# model.add(convout1)
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
# convout2 = Activation('relu')
# model.add(convout2)
# model.add(Conv1D(filters, kernel_size2,padding='valid',activation='relu', strides =1,input_shape=(maxlen,embedding_dims)))
# model.add(convout2)
# model.add(MaxPooling1D(pool_size=2, strides=1))
# model.add(Dropout(0.5))

# model.add(Flatten())
# model.add(Dense(embedding_dims))
# model.add(Activation('relu'))
# model.add(Dropout(0.5))
# model.add(Dense(2))
# #print(model.get_weights())
# model.add(Activation('sigmoid'))
# # weight = model.layers[0].get_output()
# # print(weight)
# #model.compile(loss='categorical_crossentropy',
# #               optimizer='Adadelta',
# #               metrics=['accuracy'])

model.layers[5].output

model.input

#history = model.fit(x_train, Y_train,batch_size=batch_size, epochs=50,validation_data=(x_test, Y_test),verbose=2)

model_svm = Model(model.input,model.layers[5].output)
model_svm.compile(loss='categorical_crossentropy',optimizer='Adadelta',metrics=['accuracy'])

x_svm_train = model_svm.predict(x_train)#CNN gasih?untuk masuk jadi x svm train
clf = svm.LinearSVC()
x_svm_train

clf.fit(x_svm_train,y_train)

x_svm_test = model_svm.predict(x_test)#dari cnn untuk masuk menjadi svm test
# print(x_svm_test)
y_cnn_test=[]
for i in x_svm_test:
  if(np.argmax(i)==0):
    y_cnn_test.append(-1)
  else:
    y_cnn_test.append(np.argmax(i))
# y_cnn

y_train[y_train==1].shape

predict_svm_train=clf.predict(x_svm_train)#prediksi svm train? (hasil svm)
predict_svm_train

y_cnn_train=[]
for i in x_svm_train:
  if(np.argmax(i)==0):
    y_cnn_train.append(-1)
  else:
    y_cnn_train.append(np.argmax(i))
y_cnn_train
##data athyah
# for i in x_svm_train:
#   if(np.argmax(i)==0):
#     y_cnn_train.append(-1)
#   elif(np.argmax(i)==1):
#     y_cnn_train.append(0)
#   else:
#     y_cnn_train.append(1)
# y_cnn_train

# pd.DataFrame(confusion_matrix(predict_svm_train,y_train)).transpose()

predict_svm_test=clf.predict(x_svm_test) #prediksi svm test (hasil svm)
predict_svm_test

confusion_matrix(y_train,y_cnn_train)

confusion_matrix(y_test,y_cnn_test)

print(classification_report(y_train,y_cnn_train))

print(classification_report(y_test,y_cnn_test))

confusion_matrix(y_train,predict_svm_train)

confusion_matrix(y_test,predict_svm_test)

print(classification_report(y_train,predict_svm_train))

print(classification_report(y_test,predict_svm_test))

clf.score(x_svm_test,y_test)

auc = roc_auc_score(y_test,predict_svm_test)
auc

print(metrics.accuracy_score(y_test, predict_svm_test))

filepath="weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]
# Fit the model
#model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)

history = model_svm.fit(x_train, Y_train,batch_size=batch_size, epochs=200,validation_data=(x_test, Y_test),callbacks=callbacks_list,verbose=2)

plt.title('Accuracy Graphic Plot')
plt.plot(history.history['acc'], label='train')
plt.plot(history.history['val_acc'], label='test')
plt.legend()
plt.show()

k_range = range(1,31)  #1-30
k_score = []
for k in k_range:
    clf = svm.LinearSVC()
    score = cross_val_score(clf,  x_svm_test,y_test, cv=10, scoring='accuracy') 
    k_score.append(score.mean())
print(k_score)

import matplotlib.pyplot as plt
plt.plot(k_range, k_score)
plt.xlabel('Nomor K')
plt.ylabel('score K')
plt.show()

C_range=list(np.arange(0.1,6,0.1))
acc_score=[]
for c in C_range:
    svc = SVC(kernel='linear', C=c)
    scores = cross_val_score(svc, x_svm_test,y_test, cv=10, scoring='accuracy')
    acc_score.append(scores.mean())
print(acc_score)

C_values=list(np.arange(0.1,6,0.1))
# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)
plt.plot(C_values,acc_score)
plt.xticks(np.arange(0.0,6,0.3))
plt.xlabel('Value of C for SVC ')
plt.ylabel('Cross-Validated Accuracy')

model_svm.summary()

#  history = model.fit(x_train, Y_train,
#          batch_size=batch_size,
#          epochs=5,
#          validation_data=(x_test, Y_test))

plt.title('Loss Graphic Plot')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

