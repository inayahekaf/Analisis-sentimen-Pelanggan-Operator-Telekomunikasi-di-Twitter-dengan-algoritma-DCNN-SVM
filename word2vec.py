# -*- coding: utf-8 -*-
"""word2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12m7XMfNircSnjYxKaVJuQMwwRMjexTCS
"""

import os
from google.colab import drive
drive.mount('/content/gdrive')
data_dir=os.path.join('gdrive','My Drive','Tweepy','BISMILLAH TA','hasil pelabelan')
data_file=os.path.join(data_dir, 'indosat.csv')

import os
from google.colab import drive
drive.mount('/content/gdrive')
data_dir=os.path.join('gdrive','My Drive','Tweepy','BISMILLAH TA')
data_file=os.path.join(data_dir, 'bismillah.txt')

with open ('gdrive/My Drive/tweepy/BISMILLAH TA/data/new_scrapping/1000data.csv','r',encoding='ISO-8859-1') as f:
  text=f.readlines()
print(text)

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
def token(test_result):
  tokenized_sents = [word_tokenize(i) for i in text]
  for i in tokenized_sents:
      print (i)
  return tokenized_sents

token = token(text)

#Word2Vec parameter
from gensim.models.word2vec import Word2Vec
num_features = 100
min_word_count = 2
num_workers = 2
window_size = 7
subsampling = 1e-3

model = Word2Vec(token, workers = num_workers, size = num_features, min_count = min_word_count, window = window_size, sample = subsampling)

vocab_w2v ='gdrive/My Drive/tweepy/BISMILLAH TA/data/data_word2vec/model_w2v_databaru.model'
model.save(vocab_w2v)

import random
from gensim.models.word2vec import Word2Vec
#word_vectors = Word2Vec.load('gdrive/My Drive/tweepy/BISMILLAH TA/data/data_word2vec/model_w2v_telkomsel.model')
word_vectors = Word2Vec.load('gdrive/My Drive/tweepy/BISMILLAH TA/data/data_word2vec/model_w2v_databaru.model')

vector = word_vectors.wv['jaringan']

vector

len(vector)

sentences = word_vectors['jaringan','jelek']

sentences

sentences.shape

